跳至內容
主選單 主選單
移至側邊欄utton
隱藏utton
導覽
首頁 分類索引 特色內容 新聞動態 近期變更 隨機條目 資助維基百科
說明
說明 維基社群 方針與指引 互助客棧 知識問答 字詞轉換 IRC即時聊天 聯絡我們 關於維基百科
搜尋
搜尋utton
外觀
建立帳號
登入
個人工具 建立帳號 登入
用於已登出編輯者的頁面 了解更多
貢獻 討論
目次
移至側邊欄utton
隱藏utton
序言
1
命名
2
概述
3
爬蟲策略
切換 爬蟲策略 子章節
utton
3.1
選擇策略
3.1.1
連結跟隨限制
3.1.2
URL規格化
3.1.3
路徑上移爬取
3.1.4
主題爬取
3.2
重新訪問策略
3.3
平衡禮貌策略
3.4
並列策略
4
另見
5
參考文獻
切換目次
網路爬蟲
47 種語言 Afrikaans العربية الدارجة Azərbaycanca Boarisch Català Čeština Cymraeg Deutsch Ελληνικά English Español Eesti Euskara فارسی Suomi Français עברית Hrvatski Magyar Հայերեն Interlingua Bahasa Indonesia Italiano 日本語 한국어 Lietuvių Latviešu Олык марий Bahasa Melayu Nedersaksies Nederlands Norsk nynorsk Norsk bokmål Polski Português Runa Simi Română Русский Simple English Српски / srpski Svenska தமிழ் ไทย Türkçe Українська 文言 編輯連結
條目 討論
臺灣正體 不转换 简体 繁體 大陆简体 香港繁體 澳門繁體 大马简体 新加坡简体 臺灣正體
閱讀 編輯 檢視歷史
工具 工具
移至側邊欄utton
隱藏utton
操作
閱讀 編輯 檢視歷史
一般
連結至此的頁面 相關變更 上傳檔案 特殊頁面 固定連結 頁面資訊 引用此頁面 取得短網址 下載QR碼 維基數據項目
列印/匯出
下載為PDF 可列印版
其他專案
外觀
移至側邊欄utton
隱藏utton
維基百科，自由的百科全書
本條目存在以下問題 ，請協助 改善本條目 或在 討論頁 此條目需要 精通或熟悉相關主題的編者 參與及協助編輯。 ( 2015年3月15日 請 邀請 適合的人士 改善本條目 。更多的細節與詳情請參見 討論頁 此條目 需要補充更多 來源 。 ( 2015年3月15日 請協助補充多方面 可靠來源 以 改善這篇條目 ， 無法查證 的內容可能會因為 異議提出 而被移除。 致使用者：請搜尋一下條目的標題（來源搜尋： "網路爬蟲" 網頁 、 新聞 、 書籍 、 學術 、 圖像 ），以檢查網路上是否存在該主題的更多可靠來源（ 判定指引 此條目 可參照 英語維基百科 相應條目來擴充 。 若您熟悉來源語言和主題，請協助 參考外語維基百科擴充條目 。請勿直接提交機械翻譯，也不要翻譯不可靠、低品質內容。依 版權協議 ，譯文需 在編輯摘要註明來源 {{ Translated page
網路爬蟲 （英語： web crawler ），也叫 網路蜘蛛 （ spider ），是一種用來自動瀏覽 全球資訊網 的 網路機器人 。其目的一般為編纂 網路索引 。
網路 搜尋引擎 等站點通過爬蟲軟體更新自身的 網站內容 （ 英語 ： Web content ） 或其對其他網站的索引。網路爬蟲可以將自己所訪問的頁面儲存下來，以便搜尋引擎事後生成 索引 （ 英語 ： Index (search engine) ） 供使用者搜尋。
爬蟲造訪網站的過程會消耗目標系統資源。不少網路系統並不默許爬蟲工作。因此在訪問大量頁面時，爬蟲需要考慮到規劃、負載，還需要講「禮貌」。 不願意被爬蟲訪問、被爬蟲主人知曉的公開站點可以使用 robots.txt 檔案之類的方法避免訪問。這個檔案可以要求 機器人 （ 英語 ： Software agent ） 只對 網站 的一部分進行索引，或完全不作處理。
網際網路上的頁面極多，即使是最大的爬蟲系統也無法做出完整的索引。因此在公元2000年之前的全球資訊網出現初期，搜尋引擎經常找不到多少相關結果。現在的搜尋引擎在這方面已經進步很多，能夠即刻給出高素質結果。
爬蟲還可以驗證 超連結 和 HTML 代碼，用於 網路抓取 （參見 資料驅動編程 ）。
命名 [ 編輯 ]
網路爬蟲也可稱作網路蜘蛛 1 、螞蟻、自動索引程式（ automatic indexer ） 2 FOAF （ 英語 ： FOAF (software) ） 軟體中）稱為網路疾走（ web scutter ）。 3
概述 [ 編輯 ]
網路爬蟲始於一張被稱作種子的統一資源位址（URL）列表。當網路爬蟲訪問這些統一資源定位器時，它們會甄別出頁面上所有的超連結，並將它們寫入一張「待訪列表」，即所謂 爬行疆域 （ 英語 ： crawl frontier ） 。此疆域上的URL將會被按照一套策略迴圈來訪問。如果爬蟲在執行的過程中複製歸檔和儲存網站上的資訊，這些檔案通常儲存，使他們可以較容易的被檢視。閱讀和瀏覽他們儲存的網站上並即時更新的資訊，這些被儲存的 網頁 又被稱為「快照」。越大容量的網頁意味著網路爬蟲只能在給予的時間內下載越少部分的網頁，所以要優先考慮其下載。高變化率意味著網頁可能已經被更新或者被取代。一些伺服器端軟體生成的URL（統一資源定位符）也使得網路爬蟲很難避免檢索到重複內容。
但是 網際網路 的資源卷帙浩繁，這也意味著網路爬蟲只能在一定時間內下載有限數量的網頁，因此它需要衡量優先順序的下載方式。有時候網頁出現、更新和消失的速度很快，也就是說網路爬蟲下載的網頁在幾秒後就已經被修改或甚至刪除了。這些都是網路爬蟲設計師們所面臨的兩個問題。
再者， 伺服器 端軟體所生成的統一資源位址數量龐大，以致網路爬蟲難免也會採集到重複的內容。根據 超文字傳輸協定 ，無盡組合的參數所返回的頁面中，只有很少一部分確實傳回正確的內容。例如：數張快照陳列室的網站，可能通過幾個參數，讓使用者選擇相關快照：其一是通過四種方法對快照排序，其二是關於快照解析度的的三種選擇，其三是兩種檔案格式，另加一個使用者可否提供內容的選擇，這樣對於同樣的結果會有48種（4*3*2）不同的統一資源位址與其關聯。這種數學組合替網路爬蟲造成了麻煩，因為它們必須越過這些無關指令碼變化的組合，尋找不重複的內容。
爬蟲策略 [ 編輯 ]
爬蟲的實現由以下策略組成： 4
指定頁面下載的選擇策略 檢測頁面是否改變的重新訪問策略 定義如何避免網站過度訪問的約定性策略 如何部署分散式網路爬蟲的並列策略 選擇策略 [ 編輯 ] 連結跟隨限制 [ 編輯 ]
爬蟲可能只想搜尋HTML頁面而避免其他MIME 類型。為了只請求HTML資源，爬蟲在抓取整個以GET方式請求的資源之前，通過建立HTTP的HEAD請求來決定網路資源的MIME類型。為了避免發出過多的請求，爬蟲會檢查URL和只請求那些以某些字元（如.html, .htm, .asp, .aspx, .php, .jsp, .jspx 或 / ）作為字尾的URL。這個策略可能會跳過很多HTML網路資源。
為了避免掉入從網站下載無限量的URL的爬蟲陷阱，有些爬蟲還能避免請求一些帶有「?」的資源（動態生成）。不過假若網站重寫URL以簡化URL的目的，這個策略就變得不可靠了。
URL規格化 [ 編輯 ] 主條目： URL規格化
爬蟲通常使用某些URL規格化的方式以避免資源的重複爬取。URL規格化，指的是以某種一致的方式修改和標準化URL的過程。這個過程有各種各樣的處理規則，包括統一轉換為小寫、移除「.」和「..」片段，以及在非空路徑里插入斜杆。
路徑上移爬取 [ 編輯 ]
有些爬蟲希望從指定的網站中儘可能地爬取資源。而路徑上移爬蟲就是為了能爬取每個URL里提示出的每個路徑。 5 例如，給定一個Http的種子URL: http://llama.org/hamster/monkey/page.html ，要爬取 /hamster/monkey/ ， /hamster/ 和 / 。Cothey發現路徑能非常有效地爬取獨立的資源，或以某種規律無法在站內部連結接爬取到的資源。
主題爬取 [ 編輯 ] 主條目： 主題爬蟲
對於爬蟲來說，一個頁面的重要性也可以說是，給定查詢條件一個頁面相似效能起到的作用。網路爬蟲要下載相似的網頁被稱為主題爬蟲或局部爬蟲。這個主題爬蟲或局部爬蟲的概念第一次被 Filippo Menczer 6 7 8
重新訪問策略 [ 編輯 ]
網站的屬性之一就是經常動態變化，而爬取網站的一小部分往往需要花費幾個星期或者幾個月。等到網站爬蟲完成它的爬取，很多事件也已經發生了，包括增加、更新和刪除。
在搜尋引擎的角度，因為沒有檢測這些變化，會導致儲存了過期資源的代價。最常用的估價函式是新鮮度和過時性。
新鮮度：這是一個衡量抓取內容是不是準確的二元值。在時間t內，倉庫中頁面p的新鮮度是這樣定義的：
過時性:這是一個衡量本地已抓取的內容過時程度的指標。在時間t時，倉庫中頁面p的時效性的定義如下：
平衡禮貌策略 [ 編輯 ]
爬蟲相比於人，可以有更快的檢索速度和更深的層次，所以，他們可能使一個站點癱瘓。不需要說一個單獨的爬蟲一秒鐘要執行多條請求，下載大的檔案。一個伺服器也會很難回應多執行緒爬蟲的請求。
就像Koster所注意的那樣，爬蟲的使用對很多工作都是很有用的，但是對一般的社群，也需要付出代價。使用爬蟲的代價包括： 9
網路資源：在很長一段時間，爬蟲使用相當的頻寬高度並列地工作。 伺服器超載：尤其是對給定伺服器的訪問過高時。 品質糟糕的爬蟲，可能導致伺服器或者路由器癱瘓，或者會嘗試下載自己無法處理的頁面。 個人爬蟲，如果過多的人使用，可能導致網路或者伺服器阻塞。
對這些問題的局部解決方法是漫遊器排除協定（Robots exclusion protocol），也被稱為robots.txt議定書 10 ，這份協定是讓管理員指明網路伺服器的不應該爬取的約定。這個標準沒有包括重新訪問一台伺服器的間隔的建議，雖然設定訪問間隔是避免伺服器超載的最有效辦法。最近的商業搜尋引擎，如Google，Ask Jeeves，MSN和Yahoo可以在robots.txt中使用一個額外的 「Crawl-delay」參數來指明請求之間的延遲。
並列策略 [ 編輯 ] 主條目： Distributed web crawling
一個並列爬蟲是並列執行多個行程的爬蟲。它的目標是最大化下載的速度，同時儘量減少並列的開銷和下載重複的頁面。為了避免下載一個頁面兩次，爬蟲系統需要策略來處理爬蟲執行時新發現的URL，因為同一個URL位址，可能被不同的爬蟲行程抓到。
另見 [ 編輯 ] robots.txt ：放在網頁伺服器上，告知網路蜘蛛哪些頁面內容可取得或不可取得。 Scrapy 參考文獻 [ 編輯 ] ^ Spetka, Scott. The TkWWW Robot: Beyond Browsing . NCSA . 21 November . （ 原始內容
^ Kobayashi, M. &amp; Takeda, K. Information retrieval on the web . ACM Computing Surveys (ACM Press). 2000, 32 (2): 144–173 2017-03-03 . doi:10.1145/358923.358934 . （ 原始內容
^ 參見 FOAF專案wiki對「scutter」的描述 網際網路檔案館 的 存檔 ，存檔日期2009-12-13.
^ Castillo, Carlos. Effective Web Crawling (Ph.D.論文). University of Chile. 2004 2010-08-03 . （原始內容 存檔
^ Cothey, Viv. Web-crawling reliability. Journal of the American Society for Information Science and Technology. 2004, 55 (14): 1228–1238. doi:10.1002/asi.20078
^ Menczer, F. (1997). ARACHNID: Adaptive Retrieval Agents Choosing Heuristic Neighborhoods for Information Discovery （ 頁面存檔備份 ，存於 網際網路檔案館 ）. In D. Fisher, ed., Machine Learning: Proceedings of the 14th International Conference (ICML97). Morgan Kaufmann
^ Menczer, F. and Belew, R.K. (1998). Adaptive Information Agents in Distributed Textual Environments （ 頁面存檔備份 ，存於 網際網路檔案館 ）. In K. Sycara and M. Wooldridge (eds.) Proc. 2nd Intl. Conf. on Autonomous Agents (Agents '98). ACM Press
^ Chakrabarti, S., van den Berg, M., and Dom, B. (1999). Focused crawling: a new approach to topic-specific web resource discovery . Computer Networks, 31(11–16):1623–1640.
^ E. G. Coffman Jr; Zhen Liu; Richard R. Weber. Optimal robot scheduling for Web search engines. Journal of Scheduling. 1998, 1 (1): 15–29.
^ Koster, M. (1996). A standard for robot exclusion （ 頁面存檔備份 ，存於 網際網路檔案館 ）.
權威控制資料庫 ：各地 德國
取自「 https://zh.wikipedia.org/w/index.php?title=網路爬蟲&amp;oldid=78409238 」 分類 軟體 網絡機器人 網路搜尋引擎 網絡爬蟲 Webarchive模板wayback連結 自2015年3月需要專業人士關注的頁面 所有需要專家關注的頁面 其他需要專家關注的頁面 自2015年3月需補充來源的條目 拒絕當選首頁新條目推薦欄目的條目 需要從英語維基百科翻譯的條目 含有多個問題的條目 含有英語的條目 包含GND標識符的維基百科條目
本頁面最後修訂於2023年8月6日 (星期日) 23:41。 本站的全部文字在 創用CC 署名-相同方式分享 4.0協議 之條款下提供，附加條款亦可能應用。（請參閱 使用條款 ）
Wikipedia&reg;和維基百科標誌是 維基媒體基金會 的註冊商標；維基&trade;是維基媒體基金會的商標。
維基媒體基金會是按美國國內稅收法501(c)(3)登記的 非營利慈善機構 。 隱私權政策 關於維基百科 免責聲明 行為準則 開發人員 統計 Cookie 聲明 行動版檢視
ody